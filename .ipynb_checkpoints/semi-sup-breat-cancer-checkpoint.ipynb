{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b59fe74-29c1-4024-bc3f-4aa3d56643e2",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'>Loading and Scaling the Dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7864c17-a384-4f08-a3e2-4a1c483d470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Loading the data and segregating the train and test sets.\n",
    "X,y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.8, random_state=42)\n",
    "\n",
    "# Scaling the training data.\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c314911-5c0c-4643-9c25-cfe7605a470c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 30)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you observe the dataset's shape, it is visible that we have a significant number of features.\n",
    "# Perhaps not all of them are essential for the models' creation. \n",
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa996a21-219d-4c56-a8d2-95391506d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason of choosing XGBoost as our feature selection classifier was its usual high performance in most of the projects and also\n",
    "# the fact that it presents the convenient 'feature_importances' attribute.\n",
    "\n",
    "from keras_tuner import SklearnTuner, HyperParameters\n",
    "from keras_tuner.oracles import BayesianOptimization\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Constructing the algorithm.\n",
    "def build_model(hp):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators = hp.Int('n_estimators', min_value=30, max_value=100, step=10),\n",
    "        max_depth = hp.Int('max_depth', min_value=2, max_value=4, step=1),\n",
    "        gamma = hp.Float('gamma', min_value=.05, max_value=.5, step=.05),\n",
    "        colsample_bytree = hp.Float('colsample_bytree', min_value=.1, max_value=.5, step=.1)\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef527f72-f295-4aae-a483-bc33125266d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's use K-Means\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 'n_clusters' holds the different 'n_clusters' values that will be used; 'silhouette' keeps the corresponding Silhouette Scores achieved\n",
    "# in each iteration.\n",
    "n_clusters = []\n",
    "silhouetes = []\n",
    "for i in range(2,11):\n",
    "    predictions = KMeans(n_clusters=i, random_state=42).fit_predict(X_train_scaled)\n",
    "    silhouette = silhouette_score(X_train_scaled, predictions)\n",
    "    n_clusters.append(i)\n",
    "    silhouetes.append(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6815032d-1179-47c1-9857-c2aac53ad031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What as the best 'n_clusters' number?\n",
    "idx = np.argmax(silhouetes)\n",
    "best_n_clusters = n_clusters[idx]\n",
    "\n",
    "best_n_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839e1e9-4a2d-4667-872a-9e6e379f00e3",
   "metadata": {},
   "source": [
    "<p style='color:red'> Prosseguir com a montagem da Otimização Bayesiana. Após isso, ficará mais fácil analisar os dados e remover possíveis outliers.</p>\n",
    "<p> https://keras.io/api/keras_tuner/tuners/sklearn/</p>\n",
    "<p>https://keras.io/guides/keras_tuner/tailor_the_search_space/ </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
