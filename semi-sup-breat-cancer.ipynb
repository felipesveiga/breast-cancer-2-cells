{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b59fe74-29c1-4024-bc3f-4aa3d56643e2",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'>Loading and Scaling the Dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c512c2ba-eed6-4735-94e1-7d1003d9d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Loading the data..\n",
    "breast_cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(np.concatenate((breast_cancer['data'], breast_cancer['target'].reshape(-1,1)), axis=1), \n",
    "             columns=np.append(breast_cancer['feature_names'], 'target'))\n",
    "\n",
    "# Segregating the independent variables from the dependent ones.\n",
    "X,y = df.iloc[:, :-1], df.iloc[:, -1].astype('int')\n",
    "\n",
    "# Finally, splitting the training and test sets and scaling the numbers.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f5fc4-daf7-4510-a94a-8375051eccb1",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Outlier Detection and Removal</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "90e42e39-e137-4a44-af41-4627b934d8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.44075296, -0.43531947, -1.36208497, ...,  0.9320124 ,\n",
       "         2.09724217,  1.88645014],\n",
       "       [ 1.97409619,  1.73302577,  2.09167167, ...,  2.6989469 ,\n",
       "         1.89116053,  2.49783848],\n",
       "       [-1.39998202, -1.24962228, -1.34520926, ..., -0.97023893,\n",
       "         0.59760192,  0.0578942 ],\n",
       "       ...,\n",
       "       [ 0.04880192, -0.55500086, -0.06512547, ..., -1.23903365,\n",
       "        -0.70863864, -1.27145475],\n",
       "       [-0.03896885,  0.10207345, -0.03137406, ...,  1.05001236,\n",
       "         0.43432185,  1.21336207],\n",
       "       [-0.54860557,  0.31327591, -0.60350155, ..., -0.61102866,\n",
       "        -0.3345212 , -0.84628745]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#train = np.concatenate((X_train_scaled, y_train.reshape(-1,1)), axis=1)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa996a21-219d-4c56-a8d2-95391506d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason of choosing XGBoost as our feature selection classifier was its usual high performance in most of the projects and also\n",
    "# the fact that it presents the convenient 'feature_importances' attribute.\n",
    "\n",
    "from keras_tuner import SklearnTuner, HyperParameters\n",
    "from keras_tuner.oracles import BayesianOptimization\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Constructing the algorithm.\n",
    "def build_model(hp):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators = hp.Int('n_estimators', min_value=30, max_value=100, step=10),\n",
    "        max_depth = hp.Int('max_depth', min_value=2, max_value=4, step=1),\n",
    "        gamma = hp.Float('gamma', min_value=.05, max_value=.5, step=.05),\n",
    "        colsample_bytree = hp.Float('colsample_bytree', min_value=.1, max_value=.5, step=.1)\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ef527f72-f295-4aae-a483-bc33125266d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's use K-Means\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 'n_clusters' holds the different 'n_clusters' values that will be used; 'silhouette' keeps the corresponding Silhouette Scores achieved\n",
    "# in each iteration.\n",
    "n_clusters = []\n",
    "silhouetes = []\n",
    "for i in range(2,11):\n",
    "    predictions = KMeans(n_clusters=i, random_state=42).fit_predict(X_train_scaled)\n",
    "    silhouette = silhouette_score(X_train_scaled, predictions)\n",
    "    n_clusters.append(i)\n",
    "    silhouetes.append(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6815032d-1179-47c1-9857-c2aac53ad031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What as the best 'n_clusters' number?\n",
    "idx = np.argmax(silhouetes)\n",
    "best_n_clusters = n_clusters[idx]\n",
    "\n",
    "best_n_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839e1e9-4a2d-4667-872a-9e6e379f00e3",
   "metadata": {},
   "source": [
    "<p style='color:red'> Fazer a remoção de outliers para depois montar a Bayesian Opt</p>\n",
    "<p> https://keras.io/api/keras_tuner/tuners/sklearn/</p>\n",
    "<p>https://keras.io/guides/keras_tuner/tailor_the_search_space/ </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
